{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86b7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths in the format (action, next_state)\n",
    "first_path = [(0, 1), (1, 1), (2, 1), (3, 0), (2, 2), (3, 2), (3, 2), (0, 4), (2, 5)]\n",
    "second_path = [(0, 2), (0, 4), (3, 2), (3, 2), (3, 4), (2, 5)]\n",
    "third_path = [(1, 0), (0, 2), (0, 4), (0, 4), (0, 4), (0, 4), (1, 2), (0, 4), (0, 4), (2, 5)]\n",
    "third_path = [(1, 0), (0, 2), (2, 4), (0, 4), (1, 4), (3, 4), (1, 2), (0, 4), (2, 4), (2, 5)]\n",
    "fourth_path = [(0, 2), (2, 3), (0, 5)]\n",
    "fifth_path = [(3, 0), (0, 0), (0, 2), (0, 4), (3, 4), (1, 2), (0, 4), (0, 4), (0, 4), (1, 2), (0, 2), (3, 2), (2, 3), (0, 5)]\n",
    "\n",
    "paths = [first_path, second_path, third_path, fourth_path, fifth_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41723900",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2d33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(Q, curr_state, R, action, next_state, gamma, alpha):\n",
    "    # bellman equation\n",
    "    Q_estimated = R[curr_state][next_state] +  gamma * max(Q[next_state])\n",
    "\n",
    "    Q[curr_state, action] = Q[curr_state, action] + alpha * (Q_estimated - Q[curr_state, action])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3bc77",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a47448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# R[state][next_state], i.e, reward gained going from 'state' to 'next_state'\n",
    "R = np.full((6, 6), -1)\n",
    "\n",
    "# hitting the wall (main diagonal)\n",
    "R[0, 0] = -10\n",
    "R[1, 1] = -10\n",
    "R[2, 2] = -10\n",
    "R[3, 3] = -10\n",
    "R[4, 4] = -10\n",
    "\n",
    "# going to the terminal state\n",
    "R[:, 5] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c4042",
   "metadata": {},
   "source": [
    "### Q matrix and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6917ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial Q\n",
    "Q = np.zeros((6, 4))\n",
    "\n",
    "#parameters\n",
    "alpha = 0.5\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525108aa",
   "metadata": {},
   "source": [
    "### Update Q and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd11670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Trajetória 1\n",
      "[[-0.5  0.  -0.5  0. ]\n",
      " [ 0.  -5.  -5.  -0.5]\n",
      " [-0.5  0.   0.  -7.5]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   5.   0. ]\n",
      " [ 0.   0.   0.   0. ]]\n",
      "\n",
      "RG 10\n",
      "DW UP\n",
      "DW UP\n",
      "\n",
      "- Trajetória 2\n",
      "[[-0.75    0.     -0.5     0.    ]\n",
      " [ 0.     -5.     -5.     -0.5   ]\n",
      " [ 1.75    0.      0.     -1.9375]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      7.5     0.375 ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "\n",
      "RG 10\n",
      "UP UP\n",
      "DW UP\n",
      "\n",
      "- Trajetória 3\n",
      "[[ 0.     -5.     -0.5     0.    ]\n",
      " [ 0.     -5.     -5.     -0.5   ]\n",
      " [ 4.125   0.      3.25   -1.9375]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [-1.25    0.5     6.25   -1.0625]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "\n",
      "RG 10\n",
      "UP UP\n",
      "UP UP\n",
      "\n",
      "- Trajetória 4\n",
      "[[ 1.5625 -5.     -0.5     0.    ]\n",
      " [ 0.     -5.     -5.     -0.5   ]\n",
      " [ 4.125   0.      1.125  -1.9375]\n",
      " [ 5.      0.      0.      0.    ]\n",
      " [-1.25    0.5     6.25   -1.0625]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "\n",
      "RG 10\n",
      "UP UP\n",
      "UP UP\n",
      "\n",
      "- Trajetória 5\n",
      "[[-0.15625 -5.      -0.5     -4.21875]\n",
      " [ 0.      -5.      -5.      -0.5    ]\n",
      " [-0.03125  0.       2.5625  -5.40625]\n",
      " [ 7.5      0.       0.       0.     ]\n",
      " [-3.125    3.03125  6.25    -2.40625]\n",
      " [ 0.       0.       0.       0.     ]]\n",
      "\n",
      "RG 10\n",
      "RG UP\n",
      "UP UP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show Q matrix after each path\n",
    "for i in range(len(paths)):\n",
    "    # initial state\n",
    "    curr_state = 0\n",
    "\n",
    "    # updating Q\n",
    "    for action, next_state in paths[i]:\n",
    "        Q = Q_learning(Q, curr_state, R, action, next_state, gamma, alpha)\n",
    "        curr_state = next_state\n",
    "\n",
    "    # building policy\n",
    "    actions = {0: \"UP\", 1: \"DW\", 2: \"RG\", 3: \"LF\"}\n",
    "    policy = []\n",
    "    for state in Q:\n",
    "        # choosing the better action at each state\n",
    "        policy.append(actions[np.argmax(state)])\n",
    "\n",
    "    # output\n",
    "    print_policy = f\"{policy[4]} 10\\n{policy[2]} {policy[3]}\\n{policy[0]} {policy[1]}\"\n",
    "    print(f\"- Trajetória {i + 1}\\n{Q}\\n\\n{print_policy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
